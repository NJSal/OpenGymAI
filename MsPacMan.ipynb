{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoIZabzgC5FL8nxg1t2yrw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NJSal/OpenGymAI/blob/main/MsPacMan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1p-0Hm-lt_D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "from collections import deque\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "    resized = cv2.resize(img, (32,32), interpolation = cv2.INTER_AREA)\n",
        "    resized = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY)\n",
        "    resized = np.divide(resized, 255)\n",
        "    resized = np.reshape(resized, resized.shape + (1,))\n",
        "    return resized\n",
        "\n",
        "\n",
        "#convolutional layer then a dense layer to it\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, env_inp_shape, actions):\n",
        "        self.model = keras.Sequential()\n",
        "        self.model.add(keras.layers.Conv2D(8,(3,3),padding='same', activation = 'relu', input_shape=env_inp_shape))\n",
        "        # self.model.add(keras.layers.Conv2D(8,(3,3),(2,2),padding='same', activation = 'relu'))\n",
        "        self.model.add(keras.layers.Flatten())\n",
        "        self.model.add(keras.layers.Dense(32, activation='selu', kernel_initializer='lecun_normal'))\n",
        "        self.model.add(keras.layers.Dense(32, activation='selu', kernel_initializer='lecun_normal'))\n",
        "        self.model.add(keras.layers.Dense(actions, activation='linear'))\n",
        "        self.model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.0001))\n",
        "\n",
        "\n",
        "\n",
        "        self.target_model = keras.Sequential()\n",
        "        self.target_model.add(keras.layers.Conv2D(8,(3,3),padding='same', activation = 'relu', input_shape=env_inp_shape))\n",
        "        # self.target_model.add(keras.layers.Conv2D(8,(3,3),(2,2),padding='same', activation = 'relu'))\n",
        "        self.target_model.add(keras.layers.Flatten())\n",
        "        self.target_model.add(keras.layers.Dense(32, activation='selu', kernel_initializer='lecun_normal', input_shape=env_inp_shape))\n",
        "        self.target_model.add(keras.layers.Dense(32, activation='selu', kernel_initializer='lecun_normal'))\n",
        "        self.target_model.add(keras.layers.Dense(actions))\n",
        "        \n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "\n",
        "        self.Transitions = deque(maxlen=1000)\n",
        "        self.batch_size = 32\n",
        "        self.batchs = 1\n",
        "        self.gamma = 0.99\n",
        "        self.decay = 0.99999                   \n",
        "        self.epsilon = 1\n",
        "        self.actions = range(actions)\n",
        "        self.action_size = actions\n",
        "\n",
        "\n",
        "    def __call__(self, state):\n",
        "        # if False:\n",
        "        if random.random() < self.epsilon:\n",
        "            pred = random.choice(self.actions)\n",
        "        else:\n",
        "            pred = self.model(np.array([state]), training=True)[0]\n",
        "            # print(pred)\n",
        "            pred = tf.argmax(pred).numpy()\n",
        "            # print(pred)\n",
        "        return pred\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        indices = np.random.choice(len(self.Transitions), self.batchs*self.batch_size, replace=False)\n",
        "        for i in range(0, self.batchs*self.batch_size, self.batch_size):\n",
        "            states, actions, rewards, next_states, dones = zip(*[self.Transitions[x] for x in indices[i:i+self.batch_size]])\n",
        "            preds           = self.model(np.array(states), training=False)\n",
        "            next_values     = self.target_model(np.array(next_states))\n",
        "            #next_values     = self.model(np.array(next_states), training=False)\n",
        "            max_next_values = tf.reduce_max(next_values, axis=-1)\n",
        "            max_next_values = rewards + self.gamma * max_next_values * dones\n",
        "            indx_actions    = tf.concat([tf.expand_dims(tf.range(preds.shape[0]), axis=-1), tf.expand_dims(actions, axis=-1)], axis=-1)\n",
        "\n",
        "            targets         = tf.tensor_scatter_nd_update(preds, indx_actions, max_next_values)\n",
        "            self.model.fit(np.array(states), targets, verbose=False)\n"
      ],
      "metadata": {
        "id": "3hFcFoKel9xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MsPacMan-v0\")\n",
        "n_games = 1000\n",
        "\n",
        "agent = Agent(env_inp_shape=(32,32,1), actions=env.action_space.n)\n",
        "avg_reward = None\n",
        "steps = 1\n",
        "\n",
        "for episode in range(n_games):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "    state = preprocess(state) #take image and transform it \n",
        "    done = False\n",
        "\n",
        "    while True:\n",
        "        env.render()\n",
        "\n",
        "        action = agent(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = preprocess(next_state)\n",
        "        \n",
        "\n",
        "        reward = -1 if done else reward\n",
        "\n",
        "        agent.Transitions.append([state, action, reward, next_state, int(not done)])\n",
        "        if len(agent.Transitions) > agent.batchs*agent.batch_size:\n",
        "            if steps%1000==0:\n",
        "                agent.target_model.set_weights(agent.model.get_weights())\n",
        "                print('updated')\n",
        "            agent.train()\n",
        "            agent.epsilon *= agent.decay if agent.epsilon > 0.05 else 1\n",
        "\n",
        "        ep_reward += reward\n",
        "        steps+=1\n",
        "\n",
        "        state=next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        \n",
        "    avg_reward = ep_reward if avg_reward == None else avg_reward * 0.99 + ep_reward * 0.01\n",
        "    print('\\nEpisode: %d | Episode Reward: %i | Average Reward: %f | Epsilon: %f' % (episode, ep_reward, avg_reward, agent.epsilon))\n",
        "\n",
        "print('Done:', done)"
      ],
      "metadata": {
        "id": "VrBgccZomZnf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}