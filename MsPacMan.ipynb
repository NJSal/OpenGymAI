{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoIZabzgC5FL8nxg1t2yrw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NJSal/OpenGymAI/blob/main/MsPacMan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1p-0Hm-lt_D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "from collections import deque\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "    resized = cv2.resize(img, (32,32), interpolation = cv2.INTER_AREA)\n",
        "    resized = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY)\n",
        "    resized = np.divide(resized, 255)\n",
        "    resized = np.reshape(resized, resized.shape + (1,))\n",
        "    return resized\n",
        "\n",
        "\n",
        "#convolutional layer then a dense layer to it\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, env_inp_shape, actions):\n",
        "        self.model = keras.Sequential()\n",
        "        self.model.add(keras.layers.Conv2D(8,(3,3),padding='same', activation = 'relu', input_shape=env_inp_shape))\n",
        "        # self.model.add(keras.layers.Conv2D(8,(3,3),(2,2),padding='same', activation = 'relu'))\n",
        "        self.model.add(keras.layers.Flatten())\n",
        "        self.model.add(keras.layers.Dense(32, activation='selu', kernel_initializer='lecun_normal'))\n",
        "        self.model.add(keras.layers.Dense(32, activation='selu', kernel_initializer='lecun_normal'))\n",
        "        self.model.add(keras.layers.Dense(actions, activation='linear'))\n",
        "        self.model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.0001))\n",
        "\n",
        "\n",
        "\n",
        "        self.target_model = keras.Sequential()\n",
        "        self.target_model.add(keras.layers.Conv2D(8,(3,3),padding='same', activation = 'relu', input_shape=env_inp_shape))\n",
        "        # self.target_model.add(keras.layers.Conv2D(8,(3,3),(2,2),padding='same', activation = 'relu'))\n",
        "        self.target_model.add(keras.layers.Flatten())\n",
        "        self.target_model.add(keras.layers.Dense(32, activation='selu', kernel_initializer='lecun_normal', input_shape=env_inp_shape))\n",
        "        self.target_model.add(keras.layers.Dense(32, activation='selu', kernel_initializer='lecun_normal'))\n",
        "        self.target_model.add(keras.layers.Dense(actions))\n",
        "        \n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "\n",
        "        self.Transitions = deque(maxlen=1000)\n",
        "        self.batch_size = 32\n",
        "        self.batchs = 1\n",
        "        self.gamma = 0.99\n",
        "        self.decay = 0.99999                   \n",
        "        self.epsilon = 1\n",
        "        self.actions = range(actions)\n",
        "        self.action_size = actions\n",
        "\n",
        "\n",
        "    def __call__(self, state):\n",
        "        # if False:\n",
        "        if random.random() < self.epsilon:\n",
        "            pred = random.choice(self.actions)\n",
        "        else:\n",
        "            pred = self.model(np.array([state]), training=True)[0]\n",
        "            # print(pred)\n",
        "            pred = tf.argmax(pred).numpy()\n",
        "            # print(pred)\n",
        "        return pred\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        indices = np.random.choice(len(self.Transitions), self.batchs*self.batch_size, replace=False)\n",
        "        for i in range(0, self.batchs*self.batch_size, self.batch_size):\n",
        "            states, actions, rewards, next_states, dones = zip(*[self.Transitions[x] for x in indices[i:i+self.batch_size]])\n",
        "            preds           = self.model(np.array(states), training=False)\n",
        "            next_values     = self.target_model(np.array(next_states))\n",
        "            #next_values     = self.model(np.array(next_states), training=False)\n",
        "            max_next_values = tf.reduce_max(next_values, axis=-1)\n",
        "            max_next_values = rewards + self.gamma * max_next_values * dones\n",
        "            indx_actions    = tf.concat([tf.expand_dims(tf.range(preds.shape[0]), axis=-1), tf.expand_dims(actions, axis=-1)], axis=-1)\n",
        "\n",
        "            targets         = tf.tensor_scatter_nd_update(preds, indx_actions, max_next_values)\n",
        "            self.model.fit(np.array(states), targets, verbose=False)\n"
      ],
      "metadata": {
        "id": "3hFcFoKel9xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MsPacMan-v0\")\n",
        "n_games = 1000\n",
        "\n",
        "agent = Agent(env_inp_shape=(32,32,1), actions=env.action_space.n)\n",
        "avg_reward = None\n",
        "steps = 1\n",
        "\n",
        "for episode in range(n_games):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "    state = preprocess(state) #take image and transform it \n",
        "    done = False\n",
        "\n",
        "    while True:\n",
        "        env.render()\n",
        "\n",
        "        action = agent(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = preprocess(next_state)\n",
        "        \n",
        "\n",
        "        reward = -1 if done else reward\n",
        "\n",
        "        agent.Transitions.append([state, action, reward, next_state, int(not done)])\n",
        "        if len(agent.Transitions) > agent.batchs*agent.batch_size:\n",
        "            if steps%1000==0:\n",
        "                agent.target_model.set_weights(agent.model.get_weights())\n",
        "                print('updated')\n",
        "            agent.train()\n",
        "            agent.epsilon *= agent.decay if agent.epsilon > 0.05 else 1\n",
        "\n",
        "        ep_reward += reward\n",
        "        steps+=1\n",
        "\n",
        "        state=next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        \n",
        "    avg_reward = ep_reward if avg_reward == None else avg_reward * 0.99 + ep_reward * 0.01\n",
        "    print('\\nEpisode: %d | Episode Reward: %i | Average Reward: %f | Epsilon: %f' % (episode, ep_reward, avg_reward, agent.epsilon))\n",
        "\n",
        "print('Done:', done)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "VrBgccZomZnf",
        "outputId": "25595629-b1f4-4636-9000-95718be12b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameNotFound",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7f2b29876337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MsPacMan-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_games\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_inp_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No registered env with id: {id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0m_check_name_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     raise error.NameNotFound(\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;34mf\"Environment {name} doesn't exist{namespace_msg}. {suggestion_msg}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameNotFound\u001b[0m: Environment MsPacMan doesn't exist. "
          ]
        }
      ]
    }
  ]
}